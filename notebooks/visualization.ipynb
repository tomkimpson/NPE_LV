{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NPE for Stochastic Lotka-Volterra: Visualization and Analysis\n",
    "\n",
    "This notebook provides visualization and diagnostic tools for the Neural Posterior Estimation (NPE) implementation of the stochastic Lotka-Volterra model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "from simulator import gillespie_lv, simulate_lv_batch\n",
    "from utils import create_lv_prior, create_time_grid, flatten_trajectory, compute_summary_stats\n",
    "from data_generation import LVDataGenerator\n",
    "from inference import LVInference\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulator Visualization\n",
    "\n",
    "Let's first visualize some example trajectories from the Gillespie simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up simulation parameters\n",
    "prior = create_lv_prior()\n",
    "t_grid = create_time_grid(t_max=15.0, dt=0.1)\n",
    "x0 = (50, 100)\n",
    "\n",
    "# Sample some parameter sets\n",
    "n_examples = 6\n",
    "theta_examples = prior.sample((n_examples,)).numpy()\n",
    "\n",
    "print(\"Example parameter sets:\")\n",
    "param_names = ['α', 'β', 'δ', 'γ']\n",
    "for i, theta in enumerate(theta_examples):\n",
    "    param_str = ', '.join([f'{name}={val:.3f}' for name, val in zip(param_names, theta)])\n",
    "    print(f\"Set {i+1}: {param_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate trajectories\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (ax, theta) in enumerate(zip(axes, theta_examples)):\n",
    "    try:\n",
    "        # Simulate trajectory\n",
    "        _, trajectory = gillespie_lv(theta, x0, t_max=15.0, t_grid=t_grid)\n",
    "        \n",
    "        # Plot\n",
    "        ax.plot(t_grid, trajectory[:, 0], 'b-', label='Prey', linewidth=1.5)\n",
    "        ax.plot(t_grid, trajectory[:, 1], 'r-', label='Predator', linewidth=1.5)\n",
    "        \n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Population')\n",
    "        ax.set_title(f'Set {i+1}: α={theta[0]:.2f}, β={theta[1]:.3f}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "    except Exception as e:\n",
    "        ax.text(0.5, 0.5, f'Simulation failed:\\n{str(e)[:50]}...', \n",
    "               transform=ax.transAxes, ha='center', va='center')\n",
    "        ax.set_title(f'Set {i+1}: Failed')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prior Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from prior and visualize\n",
    "prior_samples = prior.sample((5000,)).numpy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (ax, name) in enumerate(zip(axes, param_names)):\n",
    "    ax.hist(prior_samples[:, i], bins=50, alpha=0.7, color='skyblue', density=True)\n",
    "    ax.set_xlabel(f'{name} value')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'Prior distribution: {name}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Prior bounds:\")\n",
    "for i, name in enumerate(param_names):\n",
    "    low = prior.support.low[i].item()\n",
    "    high = prior.support.high[i].item()\n",
    "    print(f\"{name}: [{low:.3f}, {high:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Generation Analysis\n",
    "\n",
    "Let's analyze the training data generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate small batch of training data\n",
    "generator = LVDataGenerator(\n",
    "    x0=(50, 100),\n",
    "    t_max=10.0,\n",
    "    dt=0.1,\n",
    "    use_summary_stats=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Generating sample training data...\")\n",
    "theta_sample, x_sample = generator.generate_batch(n_samples=100, batch_size=50)\n",
    "\n",
    "print(f\"Generated data shapes: theta={theta_sample.shape}, x={x_sample.shape}\")\n",
    "print(f\"Success rate: {generator.get_stats()['success_rate']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some generated trajectories\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < len(x_sample):\n",
    "        # Reshape flattened trajectory\n",
    "        trajectory = x_sample[i].numpy().reshape(-1, 2)\n",
    "        theta = theta_sample[i].numpy()\n",
    "        \n",
    "        ax.plot(t_grid, trajectory[:, 0], 'b-', label='Prey', linewidth=1.5)\n",
    "        ax.plot(t_grid, trajectory[:, 1], 'r-', label='Predator', linewidth=1.5)\n",
    "        \n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Population')\n",
    "        ax.set_title(f'Sample {i+1}: α={theta[0]:.2f}, β={theta[1]:.3f}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary Statistics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare full trajectory vs summary statistics\n",
    "generator_summary = LVDataGenerator(\n",
    "    x0=(50, 100),\n",
    "    t_max=10.0,\n",
    "    dt=0.1,\n",
    "    use_summary_stats=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "theta_test, x_summary = generator_summary.generate_batch(n_samples=50, batch_size=25)\n",
    "\n",
    "print(f\"Full trajectory dimension: {x_sample.shape[1]}\")\n",
    "print(f\"Summary statistics dimension: {x_summary.shape[1]}\")\n",
    "print(f\"Compression ratio: {x_sample.shape[1] / x_summary.shape[1]:.1f}x\")\n",
    "\n",
    "# Show example summary statistics\n",
    "print(\"\\nExample summary statistics:\")\n",
    "print(\"(mean_prey, std_prey, min_prey, max_prey, peak_time_prey, autocorr_prey,\")\n",
    "print(\" mean_pred, std_pred, min_pred, max_pred, peak_time_pred, autocorr_pred)\")\n",
    "for i in range(min(3, len(x_summary))):\n",
    "    print(f\"Sample {i+1}: {x_summary[i].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training Visualization\n",
    "\n",
    "Functions to visualize training progress (use after training a model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(training_info):\n",
    "    \"\"\"Plot training and validation loss curves.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    epochs = range(1, len(training_info['train_log_probs']) + 1)\n",
    "    \n",
    "    ax.plot(epochs, -np.array(training_info['train_log_probs']), \n",
    "           'b-', label='Training Loss', linewidth=2)\n",
    "    ax.plot(epochs, -np.array(training_info['validation_log_probs']), \n",
    "           'r-', label='Validation Loss', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Negative Log Probability')\n",
    "    ax.set_title('NPE Training Progress')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Final training loss: {-training_info['train_log_probs'][-1]:.4f}\")\n",
    "    print(f\"Final validation loss: {-training_info['validation_log_probs'][-1]:.4f}\")\n",
    "    print(f\"Training epochs: {len(training_info['train_log_probs'])}\")\n",
    "\n",
    "# Example usage (uncomment when you have trained a model):\n",
    "# training_info = {...}  # from training script\n",
    "# plot_training_curves(training_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Posterior Predictive Checks\n",
    "\n",
    "Functions for validating trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_predictive_check(inference_model, theta_true, x_obs, n_samples=100):\n",
    "    \"\"\"Perform posterior predictive check.\"\"\"\n",
    "    \n",
    "    # Sample from posterior\n",
    "    posterior_samples = inference_model.sample_posterior(x_obs, num_samples=n_samples)\n",
    "    \n",
    "    # Simulate data from posterior samples\n",
    "    t_grid = create_time_grid(10.0, 0.1)\n",
    "    x0 = (50, 100)\n",
    "    \n",
    "    predicted_trajectories = []\n",
    "    \n",
    "    for i in range(min(20, len(posterior_samples))):\n",
    "        theta_sample = posterior_samples[i].numpy()\n",
    "        try:\n",
    "            _, traj = gillespie_lv(theta_sample, x0, 10.0, t_grid)\n",
    "            predicted_trajectories.append(traj)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if len(predicted_trajectories) == 0:\n",
    "        print(\"No successful predictions generated\")\n",
    "        return\n",
    "    \n",
    "    # Plot results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Reconstruct true trajectory if using full data\n",
    "    if not inference_model.use_summary_stats:\n",
    "        true_trajectory = x_obs.numpy().reshape(-1, 2)\n",
    "        ax1.plot(t_grid, true_trajectory[:, 0], 'b-', linewidth=3, label='True (Prey)', alpha=0.8)\n",
    "        ax2.plot(t_grid, true_trajectory[:, 1], 'r-', linewidth=3, label='True (Predator)', alpha=0.8)\n",
    "    \n",
    "    # Plot predicted trajectories\n",
    "    for i, traj in enumerate(predicted_trajectories):\n",
    "        alpha = 0.1 if i > 0 else 0.3\n",
    "        label_prey = 'Predicted (Prey)' if i == 0 else None\n",
    "        label_pred = 'Predicted (Predator)' if i == 0 else None\n",
    "        \n",
    "        ax1.plot(t_grid, traj[:, 0], 'b-', alpha=alpha, label=label_prey)\n",
    "        ax2.plot(t_grid, traj[:, 1], 'r-', alpha=alpha, label=label_pred)\n",
    "    \n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Prey Population')\n",
    "    ax1.set_title('Posterior Predictive Check: Prey')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.set_ylabel('Predator Population')\n",
    "    ax2.set_title('Posterior Predictive Check: Predator')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return posterior_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_coverage(inference_model, n_tests=50, confidence_level=0.95):\n",
    "    \"\"\"Analyze posterior coverage using simulation-based calibration.\"\"\"\n",
    "    \n",
    "    prior = create_lv_prior()\n",
    "    t_grid = create_time_grid(10.0, 0.1)\n",
    "    x0 = (50, 100)\n",
    "    \n",
    "    coverage_results = []\n",
    "    \n",
    "    for i in tqdm(range(n_tests), desc=\"Coverage analysis\"):\n",
    "        # Sample true parameters\n",
    "        theta_true = prior.sample((1,)).numpy()[0]\n",
    "        \n",
    "        try:\n",
    "            # Generate synthetic observation\n",
    "            _, trajectory = gillespie_lv(theta_true, x0, 10.0, t_grid)\n",
    "            \n",
    "            if inference_model.use_summary_stats:\n",
    "                x_obs = torch.tensor(compute_summary_stats(trajectory, t_grid), dtype=torch.float32)\n",
    "            else:\n",
    "                x_obs = torch.tensor(flatten_trajectory(trajectory), dtype=torch.float32)\n",
    "            \n",
    "            # Sample from posterior\n",
    "            posterior_samples = inference_model.sample_posterior(x_obs, num_samples=1000)\n",
    "            \n",
    "            # Check coverage for each parameter\n",
    "            test_result = []\n",
    "            for j in range(4):\n",
    "                samples_j = posterior_samples[:, j].numpy()\n",
    "                alpha = 1 - confidence_level\n",
    "                lower = np.percentile(samples_j, 100 * alpha / 2)\n",
    "                upper = np.percentile(samples_j, 100 * (1 - alpha / 2))\n",
    "                \n",
    "                covered = lower <= theta_true[j] <= upper\n",
    "                test_result.append(covered)\n",
    "            \n",
    "            coverage_results.append(test_result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Test {i} failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Analyze results\n",
    "    coverage_results = np.array(coverage_results)\n",
    "    coverage_rates = np.mean(coverage_results, axis=0)\n",
    "    \n",
    "    print(f\"\\nCoverage Analysis ({confidence_level:.0%} credible intervals):\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, name in enumerate(param_names):\n",
    "        rate = coverage_rates[i]\n",
    "        print(f\"{name}: {rate:.2%} (expected: {confidence_level:.0%})\")\n",
    "    \n",
    "    overall_coverage = np.mean(coverage_rates)\n",
    "    print(f\"\\nOverall coverage: {overall_coverage:.2%}\")\n",
    "    \n",
    "    return coverage_rates, coverage_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Usage Examples\n",
    "\n",
    "Here are examples of how to use the functions above after training a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example workflow (uncomment and modify paths as needed):\n",
    "\n",
    "# 1. Load trained model\n",
    "# inference = LVInference.load_model('../models/npe_model.pkl')\n",
    "\n",
    "# 2. Generate test observation\n",
    "# theta_true = np.array([0.5, 0.025, 0.025, 0.5])\n",
    "# t_grid = create_time_grid(10.0, 0.1)\n",
    "# _, trajectory = gillespie_lv(theta_true, (50, 100), 10.0, t_grid)\n",
    "# x_obs = torch.tensor(flatten_trajectory(trajectory), dtype=torch.float32)\n",
    "\n",
    "# 3. Run posterior predictive check\n",
    "# posterior_samples = posterior_predictive_check(inference, theta_true, x_obs)\n",
    "\n",
    "# 4. Analyze coverage\n",
    "# coverage_rates, _ = analyze_coverage(inference, n_tests=20)\n",
    "\n",
    "print(\"Visualization notebook ready!\")\n",
    "print(\"Uncomment and run the examples above after training a model.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}